{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a6460d-baa3-496e-8772-2536f45ac05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Set\n",
    "import networkx as nx\n",
    "from typing import List, Tuple\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b491ba9a-b0f3-46cd-a279-e99634a85444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_extract_multiple(file_path, target_strings):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Convert lines to a NumPy array\n",
    "        data = np.array([line.strip() for line in lines])\n",
    "\n",
    "        result_2d_array = []\n",
    "\n",
    "        for target_string in target_strings:\n",
    "            # Find the index of the target string\n",
    "            start_index = np.where(data == target_string)[0]\n",
    "\n",
    "            if len(start_index) == 0:\n",
    "                print(f\"Target string '{target_string}' not found in the file.\")\n",
    "                result_2d_array.append(None)\n",
    "                continue\n",
    "\n",
    "            start_index = start_index[0]\n",
    "\n",
    "            # Find the index of the next '####'\n",
    "            end_index = np.where(data == '####')[0]\n",
    "\n",
    "            if len(end_index) == 0:\n",
    "                print(\"No '####' found after the target string.\")\n",
    "                result_2d_array.append(None)\n",
    "                continue\n",
    "\n",
    "            end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "            if len(end_index) == 0:\n",
    "                print(\"No '####' found after the target string.\")\n",
    "                result_2d_array.append(None)\n",
    "                continue\n",
    "\n",
    "            end_index = end_index[0]\n",
    "\n",
    "            # Skip the first 5 lines after the target string\n",
    "            start_index += 6\n",
    "\n",
    "            # Extract the range between the target string and '####'\n",
    "            result_array = data[start_index:end_index]\n",
    "            result_2d_array.append(result_array)\n",
    "\n",
    "        return result_2d_array\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return [None] * len(target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91637622-3e0d-4425-9628-f93d191453cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data_list, variable_names):\n",
    "    metrics_dict = {\n",
    "        'Count': [],\n",
    "        'Mean': [],\n",
    "        'Std Dev': [],\n",
    "        'Max-Min': [],\n",
    "        'Coef of Var': []  # Added Coefficient of Variation\n",
    "    }\n",
    "\n",
    "    for idx, data_array in enumerate(data_list):\n",
    "        if data_array is not None:\n",
    "            # Convert the array to numpy floats for calculations\n",
    "            data_array = data_array.astype(float)\n",
    "\n",
    "            # Calculate metrics\n",
    "            count_value = len(data_array)\n",
    "            mean_value = np.mean(data_array)\n",
    "            std_dev_value = np.std(data_array)\n",
    "            max_min_diff = np.max(data_array) - np.min(data_array)\n",
    "            coef_of_var = std_dev_value / mean_value  # Coefficient of Variation\n",
    "\n",
    "            # Append values to the metrics dictionary\n",
    "            metrics_dict['Count'].append(count_value)\n",
    "            metrics_dict['Mean'].append(mean_value)\n",
    "            metrics_dict['Std Dev'].append(std_dev_value)\n",
    "            metrics_dict['Max-Min'].append(max_min_diff)\n",
    "            metrics_dict['Coef of Var'].append(abs(coef_of_var))\n",
    "        else:\n",
    "            # If data is None, append NaN values\n",
    "            metrics_dict['Count'].append(np.nan)\n",
    "            metrics_dict['Mean'].append(np.nan)\n",
    "            metrics_dict['Std Dev'].append(np.nan)\n",
    "            metrics_dict['Max-Min'].append(np.nan)\n",
    "            metrics_dict['Coef of Var'].append(np.nan)\n",
    "\n",
    "    # Create a DataFrame from the metrics dictionary with variable names as index\n",
    "    df = pd.DataFrame(metrics_dict, index=variable_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe320c1e-ef2f-4a9a-b021-bde5e40d89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variables_coef_variation(dataframe, min_coef, max_coef):\n",
    "    \"\"\"\n",
    "    Filter variables in a DataFrame based on the standard deviation range.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The input DataFrame containing variables and their metrics.\n",
    "    - min_std_dev (float): The minimum standard deviation threshold.\n",
    "    - max_std_dev (float): The maximum standard deviation threshold.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing variables that meet the standard deviation criteria.\n",
    "    \"\"\"\n",
    "    selected_variables = dataframe[\n",
    "        (dataframe['Coef of Var'] >= min_coef) & \n",
    "        (dataframe['Coef of Var'] <= max_coef) \n",
    "    ]\n",
    "\n",
    "    return selected_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bb447c-ebcb-4bbc-97a3-dce9bcfd1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variables_by_use(dataframe: pd.DataFrame, used: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter variables in a DataFrame based on the standard deviation range.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The input DataFrame containing variables and their metrics.\n",
    "    - used (pd.DataFrame): The DataFrame containing information about the use of variables.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing variables that meet the standard deviation criteria.\n",
    "    \"\"\"\n",
    "    # Merge dataframes on index and the specified column\n",
    "    merged_df = pd.merge(dataframe, used.iloc[:, [0]], left_index=True, right_index=True)\n",
    "\n",
    "    # Filter rows where the merged column is equal to 1\n",
    "    selected_variables = merged_df[merged_df.iloc[:, -1] == 1]\n",
    "    \n",
    "        # Drop the last column before returning\n",
    "    selected_variables = selected_variables.iloc[:, :-1]\n",
    "\n",
    "    return selected_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107eaf0-63e3-4144-bfeb-3ed820445747",
   "metadata": {},
   "source": [
    "Par_Paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54f64fc-0fe5-49b1-8cd1-cd90c526e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nseg(file_path: str, target_id: int) -> List[int]:\n",
    "    '''Find upstream segments in a river network given\n",
    "    in the form of a text file between \"tosegment\" and \"#### \n",
    "    these represent the nseg index values to replace with parameters, the target ID is included in the list\n",
    "    because the gauge is assumed to be at the end of the segments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the text file containing river segment information\n",
    "    target_id: int\n",
    "        Target ID for which upstream segments are desired\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nodes: list of int\n",
    "        IDs of upstream or ancestor segments\n",
    "    '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Convert lines to a NumPy array\n",
    "    data = np.array([line.strip() for line in lines])\n",
    "\n",
    "    # Find the index of 'tosegment'\n",
    "    start_index = np.where(data == 'tosegment')[0]\n",
    "\n",
    "    if len(start_index) == 0:\n",
    "        print(\"Target string 'tosegment' not found in the file.\")\n",
    "        return []\n",
    "\n",
    "    start_index = start_index[0]\n",
    "\n",
    "    # Find the index of the next '####'\n",
    "    end_index = np.where(data == '####')[0]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'tosegment'.\")\n",
    "        return []\n",
    "\n",
    "    end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'tosegment'.\")\n",
    "        return []\n",
    "\n",
    "    end_index = end_index[0]\n",
    "\n",
    "    start_index += 5\n",
    "    \n",
    "    result_array = data[start_index:end_index]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'upstream_segments': result_array}, index=np.arange(1, len(result_array) + 1))\n",
    "    \n",
    "    # Convert index to strings\n",
    "    df.index = df.index.astype(str)\n",
    "\n",
    "    # Creating a DiGraph out of `df` object\n",
    "    riv_graph = nx.from_pandas_edgelist(df.reset_index(), source='index', target='upstream_segments', create_using=nx.DiGraph)\n",
    "    \n",
    "    # Return nodes as a list of integers, including the target_id\n",
    "    nodes = nx.ancestors(riv_graph, str(target_id))\n",
    "    nodes_list = list(map(int, nodes)) + [target_id]\n",
    "\n",
    "    return nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7e94e0-b973-44a4-9b33-e0838d29e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(list1, list2):\n",
    "    '''Find unique values from two lists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: list of int\n",
    "        First list of integers\n",
    "    list2: list of int\n",
    "        Second list of integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: list of int\n",
    "        List of unique values from both lists\n",
    "    '''\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Find unique values from both sets\n",
    "    unique_values = list(set1.symmetric_difference(set2))\n",
    "    \n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0cbe6a7-812a-4009-afd1-bcb7d2f3416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_nseg(list1, list2):\n",
    "    '''Merge two lists and find unique values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: list of int\n",
    "        First list of integers\n",
    "    list2: list of int\n",
    "        Second list of integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: list of int\n",
    "        List of unique values from both lists\n",
    "    '''\n",
    "    merged_list = list1 + list2\n",
    "    unique_values = list(set(merged_list))\n",
    "    \n",
    "    return unique_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29fef248-5159-41e3-b6be-a6d8330c4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nhru(file_path: str, nseg: list) -> List[int]:\n",
    "    ''' find the corresponding nhru index positions to the nseg\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the text file containing river segment information\n",
    "        \n",
    "    nseg: list of index positions of upstream river segments\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nhru: list of hru values corresponding to the segments\n",
    "    \n",
    "    '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Convert lines to a NumPy array\n",
    "    data = np.array([line.strip() for line in lines])\n",
    "\n",
    "    # Find the index of 'tosegment'\n",
    "    start_index = np.where(data == 'hru_segment')[0]\n",
    "\n",
    "    if len(start_index) == 0:\n",
    "        print(\"Target string 'tosegment' not found in the file.\")\n",
    "        return []\n",
    "\n",
    "    start_index = start_index[0]\n",
    "\n",
    "    # Find the index of the next '####'\n",
    "    end_index = np.where(data == '####')[0]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'tosegment'.\")\n",
    "        return []\n",
    "\n",
    "    end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'tosegment'.\")\n",
    "        return []\n",
    "\n",
    "    end_index = end_index[0]\n",
    "\n",
    "    start_index += 5\n",
    "    \n",
    "    result_array = data[start_index:end_index]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'corresponding hru': result_array}, index=np.arange(1, len(result_array) + 1))\n",
    "\n",
    "    # Find corresponding nhru values for each nseg\n",
    "    nhru_values = []\n",
    "    for seg in nseg:\n",
    "        # Find index positions where seg is present in the first column\n",
    "        indices = np.where(df['corresponding hru'] == str(seg))[0] + 1  # Adding 1 to convert to 1-based index\n",
    "        nhru_values.extend(indices)\n",
    "\n",
    "    return nhru_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1ab0e6-c879-471a-916a-ba5a2402b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nseg(array: np.ndarray, dataframe: pd.DataFrame, nhru_results: List[int], updated_strings: List[List[str]]) -> np.ndarray:\n",
    "    '''Filter the DataFrame based on the 'Count' column values of 908 or 909.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        NumPy array (to be updated).\n",
    "    dataframe: pd.DataFrame\n",
    "        DataFrame to be filtered.\n",
    "    nhru_results: List[int]\n",
    "        List of nhru values.\n",
    "    updated_strings: List[List[str]]\n",
    "        2D List to store updated strings (each row: [original_string, numerical_value, updated_string]).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_array: np.ndarray\n",
    "        Updated NumPy array with index numbers rewritten as strings.\n",
    "    '''\n",
    "    # Filter the DataFrame based on 'Count' column values\n",
    "    filtered_dataframe = dataframe[dataframe['Count'].isin([448, 449])]\n",
    "    \n",
    "    # Iterate through strings in the index of the filtered DataFrame\n",
    "    for string_to_find in filtered_dataframe.index:\n",
    "        # Find the index of 'tosegment'\n",
    "        start_index = np.where(array == string_to_find)[0]\n",
    "\n",
    "        if len(start_index) == 0:\n",
    "            print(f\"Target string '{string_to_find}' not found in the array.\")\n",
    "            continue\n",
    "\n",
    "        start_index = start_index[0]\n",
    "\n",
    "        # Find the index of the next '####'\n",
    "        end_index = np.where(array == '####')[0]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[0]\n",
    "\n",
    "        # Skip the first 4 lines for nseg\n",
    "        start_index += 5\n",
    "\n",
    "        # Save the range between start and end index to a NumPy array\n",
    "        result_array = np.arange(start_index, end_index + 1)\n",
    "\n",
    "        # Iterate through the list of nhru_results and update the corresponding indices\n",
    "        for nhru_value in nhru_results:\n",
    "            # Map nhru_value to the corresponding index in result_array\n",
    "            index_to_update = nhru_value - 1\n",
    "\n",
    "            if index_to_update < 0 or index_to_update >= len(result_array):\n",
    "                print(f\"Index value '{nhru_value}' is out of range for '{string_to_find}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Save the numerical value being overwritten\n",
    "            original_value = array[result_array[index_to_update]]\n",
    "\n",
    "            # Update the array with the new string format\n",
    "            updated_value = f'___{string_to_find}{nhru_value}_A'\n",
    "            array[result_array[index_to_update]] = updated_value\n",
    "\n",
    "            # Append the updated string to the 2D list\n",
    "            updated_strings.append([string_to_find, original_value, updated_value])\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6d114f-df6f-4d7a-98c2-152e9aef9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nhru(array: np.ndarray, dataframe: pd.DataFrame, nhru_results: List[int], updated_strings: List[List[str]]) -> np.ndarray:\n",
    "    '''Filter the DataFrame based on the 'Count' column values of 908 or 909.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        NumPy array (to be updated).\n",
    "    dataframe: pd.DataFrame\n",
    "        DataFrame to be filtered.\n",
    "    nhru_results: List[int]\n",
    "        List of nhru values.\n",
    "    updated_strings: List[List[str]]\n",
    "        2D List to store updated strings (each row: [original_string, numerical_value, updated_string]).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_array: np.ndarray\n",
    "        Updated NumPy array with index numbers rewritten as strings.\n",
    "    '''\n",
    "    # Filter the DataFrame based on 'Count' column values\n",
    "    filtered_dataframe = dataframe[dataframe['Count'].isin([908, 909])]\n",
    "    \n",
    "    # Iterate through strings in the index of the filtered DataFrame\n",
    "    for string_to_find in filtered_dataframe.index:\n",
    "        # Find the index of 'tosegment'\n",
    "        start_index = np.where(array == string_to_find)[0]\n",
    "\n",
    "        if len(start_index) == 0:\n",
    "            print(f\"Target string '{string_to_find}' not found in the array.\")\n",
    "            continue\n",
    "\n",
    "        start_index = start_index[0]\n",
    "\n",
    "        # Find the index of the next '####'\n",
    "        end_index = np.where(array == '####')[0]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[0]\n",
    "\n",
    "        # Skip the first 4 lines for nhru\n",
    "        start_index += 5\n",
    "\n",
    "        # Save the range between start and end index to a NumPy array\n",
    "        result_array = np.arange(start_index, end_index + 1)\n",
    "\n",
    "        # Iterate through the list of nhru_results and update the corresponding indices\n",
    "        for nhru_value in nhru_results:\n",
    "            # Map nhru_value to the corresponding index in result_array\n",
    "            index_to_update = nhru_value - 1\n",
    "\n",
    "            if index_to_update < 0 or index_to_update >= len(result_array):\n",
    "                print(f\"Index value '{nhru_value}' is out of range for '{string_to_find}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Save the numerical value being overwritten\n",
    "            original_value = array[result_array[index_to_update]]\n",
    "\n",
    "            # Update the array with the new string format\n",
    "            updated_value = f'___{string_to_find}{nhru_value}_A'\n",
    "            array[result_array[index_to_update]] = updated_value\n",
    "\n",
    "            # Append the updated string to the 2D list\n",
    "            updated_strings.append([string_to_find, original_value, updated_value])\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8671ad-cf58-4896-ad03-929ee4b2f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nmonths(array: np.ndarray, dataframe: pd.DataFrame, nhru_results: List[int], updated_strings: List[List[str]]) -> np.ndarray:\n",
    "    '''Filter the DataFrame based on the 'Count' column values of 908 or 909.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        NumPy array (to be updated).\n",
    "    dataframe: pd.DataFrame\n",
    "        DataFrame to be filtered.\n",
    "    nhru_results: List[int]\n",
    "        List of nhru values.\n",
    "    updated_strings: List[List[str]]\n",
    "        2D List to store updated strings (each row: [original_string, numerical_value, updated_string]).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_array: np.ndarray\n",
    "        Updated NumPy array with index numbers rewritten as strings.\n",
    "    '''\n",
    "    # Filter the DataFrame based on 'Count' column values\n",
    "    filtered_dataframe = dataframe[dataframe['Count'].isin([10908, 10909])]\n",
    "    \n",
    "    # Iterate through strings in the index of the filtered DataFrame\n",
    "    for string_to_find in filtered_dataframe.index:\n",
    "        # Find the index of 'tosegment'\n",
    "        start_index = np.where(array == string_to_find)[0]\n",
    "\n",
    "        if len(start_index) == 0:\n",
    "            print(f\"Target string '{string_to_find}' not found in the array.\")\n",
    "            continue\n",
    "\n",
    "        start_index = start_index[0]\n",
    "\n",
    "        # Find the index of the next '####'\n",
    "        end_index = np.where(array == '####')[0]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "        if len(end_index) == 0:\n",
    "            print(\"No '####' found after the target string.\")\n",
    "            continue\n",
    "\n",
    "        end_index = end_index[0]\n",
    "\n",
    "        # Skip the first 5 lines for nmonths\n",
    "        start_index += 6\n",
    "\n",
    "        # Save the range between start and end index to a NumPy array\n",
    "        result_array = np.arange(start_index, end_index + 1)\n",
    "\n",
    "        # Iterate through the list of nhru_results and update the corresponding indices\n",
    "        for nhru_value in nhru_results:\n",
    "            # Map nhru_value to the corresponding index in result_array\n",
    "            index_to_update = (nhru_value - 1) * 12\n",
    "\n",
    "            if index_to_update < 0 or index_to_update + 11 >= len(array):\n",
    "                print(f\"Index value '{nhru_value}' is out of range for '{string_to_find}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Save the numerical values being overwritten\n",
    "            original_values = array[result_array[index_to_update:index_to_update + 12]]\n",
    "\n",
    "            for i in range(12):\n",
    "                # Use letters A-L at the end instead of double digits\n",
    "                updated_value = f'___{string_to_find}{nhru_value}_{string.ascii_uppercase[i]}'\n",
    "                array[result_array[index_to_update + i]] = updated_value\n",
    "\n",
    "                # Append the updated string to the 2D list\n",
    "                updated_strings.append([string_to_find, original_values[i], updated_value])\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ebe6d8-f57b-4c77-93ac-800ebffc130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_alt_input(array: np.ndarray, dataframe: pd.DataFrame, nhru_results: List[int], updated_strings: List[List[str]]) -> pd.DataFrame:\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        NumPy array (to be updated).\n",
    "    dataframe: pd.DataFrame\n",
    "        DataFrame to be filtered.\n",
    "    nhru_results: List[int]\n",
    "        List of nhru values.\n",
    "    updated_strings: List[List[str]]\n",
    "        2D List to store updated strings (each row: [original_string, numerical_value, updated_string]).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_dataframe: pd.DataFrame\n",
    "        New DataFrame with 'nhru' as the index and columns named after the row index in the filtered DataFrame, filled with 1.\n",
    "    '''\n",
    "    # Filter the DataFrame based on 'Count' column values\n",
    "    filtered_dataframe = dataframe[dataframe['Count'].isin([10908, 10909])]\n",
    "    \n",
    "    # Create a new DataFrame with 'nhru' as the index and specified columns\n",
    "    new_dataframe = pd.DataFrame(index=nhru_results, columns=filtered_dataframe.index.astype(str))\n",
    "    \n",
    "    # Fill all values in the new DataFrame with 1\n",
    "    new_param= new_dataframe.fillna(1)\n",
    "    \n",
    "        # Extract column names as a list\n",
    "    column_names = new_dataframe.columns.tolist()\n",
    "    \n",
    "    # Iterate through each column and populate with strings\n",
    "    for column_name in new_dataframe.columns:\n",
    "        for row_index in new_dataframe.index:\n",
    "            value = f'___adjust_{column_name}_{row_index}_A'\n",
    "            new_dataframe.at[row_index, column_name] = value\n",
    "            updated_strings.append([column_name, new_dataframe.at[row_index, column_name], value])\n",
    "    \n",
    "    return new_param, new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d09adab-d558-4313-8a92-cd3c03bf9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obsin_segment(array: np.ndarray, seg: int, obs: int) -> np.ndarray:\n",
    "    '''Replace the inflow to a segment with observed flow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        NumPy array (to be updated).\n",
    "    seg: int\n",
    "        Segment to replace.\n",
    "    obs: int\n",
    "        Index of observed flow.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_array: np.ndarray\n",
    "        Updated NumPy array with index numbers replaced by observed flow.\n",
    "    '''\n",
    "\n",
    "    # Find the index of 'obsin_segment'\n",
    "    start_index = np.where(array == 'obsin_segment')[0]\n",
    "\n",
    "    if len(start_index) == 0:\n",
    "        print(\"Target string 'obsin_segment' not found in the array.\")\n",
    "        return array\n",
    "\n",
    "    start_index = start_index[0]\n",
    "\n",
    "    # Find the index of the next '####'\n",
    "    end_index = np.where(array == '####')[0]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'obsin_segment'.\")\n",
    "        return array\n",
    "\n",
    "    end_index = end_index[np.where(end_index > start_index)]\n",
    "\n",
    "    if len(end_index) == 0:\n",
    "        print(\"No '####' found after 'obsin_segment'.\")\n",
    "        return array\n",
    "\n",
    "    end_index = end_index[0]\n",
    "\n",
    "    # Skip the first 4 lines for nseg\n",
    "    start_index += 5\n",
    "\n",
    "    # Save the range between start and end index to a NumPy array\n",
    "    result_array = np.arange(start_index, end_index + 1)\n",
    "\n",
    "    # Map seg to the corresponding index in result_array\n",
    "    index_to_update = seg - 1\n",
    "\n",
    "    if index_to_update < 0 or index_to_update >= len(result_array):\n",
    "        print(f\"Index value '{seg}' is out of range for 'obsin_segment'. Skipping.\")\n",
    "        return array\n",
    "\n",
    "    # Update the array with the observed flow value\n",
    "    array[result_array[index_to_update]] = obs\n",
    "\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14983c26-c2bc-48ff-891c-52b10a0eb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ostin_table(ostin_table: pd.DataFrame, updated_strings: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Generate a new DataFrame based on specified conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ostin_table : pd.DataFrame\n",
    "        DataFrame containing 'ostin_table' data.\n",
    "\n",
    "    updated_strings : np.ndarray\n",
    "        2D array containing numerical values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_dataframe : pd.DataFrame\n",
    "        A new DataFrame generated based on the specified conditions.\n",
    "    '''\n",
    "    # Create a DataFrame from the updated_strings data\n",
    "    updated_strings_df = pd.DataFrame(updated_strings, columns=['Parameter', 'Original_Value', 'Updated_String'])\n",
    "\n",
    "    # Set the 'Parameter' column as the index for quick lookups\n",
    "    ostin_table.index.name = 'Parameter'\n",
    "    updated_strings_df.set_index('Parameter', inplace=True)\n",
    "\n",
    "    # Change the original values of 0 to 0.01\n",
    "    updated_strings_df['Original_Value'] = pd.to_numeric(updated_strings_df['Original_Value'], errors='coerce')\n",
    "    updated_strings_df['Original_Value'].replace(0, 0.01, inplace=True)\n",
    "\n",
    "    for parameter_name in updated_strings_df.index.unique():\n",
    "        # Find corresponding values in ostin_table\n",
    "        if parameter_name in ostin_table.index:\n",
    "            low_bound = ostin_table.loc[parameter_name, 'low_bound']\n",
    "            upper_bound = ostin_table.loc[parameter_name, 'upper_bound']\n",
    "\n",
    "            # Check if the low_bound is -9999\n",
    "            if low_bound == -9999:\n",
    "                original_values = updated_strings_df.loc[parameter_name, 'Original_Value']\n",
    "                updated_strings_df.loc[parameter_name, 'Low_Bound'] = 0.8 * original_values\n",
    "            else:\n",
    "                updated_strings_df.loc[parameter_name, 'Low_Bound'] = low_bound\n",
    "\n",
    "            # Check if the upper_bound is -9999\n",
    "            if upper_bound == -9999:\n",
    "                original_values = updated_strings_df.loc[parameter_name, 'Original_Value']\n",
    "                updated_strings_df.loc[parameter_name, 'Upper_Bound'] = 1.2 * original_values\n",
    "            else:\n",
    "                updated_strings_df.loc[parameter_name, 'Upper_Bound'] = upper_bound\n",
    "\n",
    "    # Set the \"Updated_String\" column as the index\n",
    "    updated_strings_df = updated_strings_df.set_index('Updated_String')\n",
    "\n",
    "    # Add three new columns filled with 'none'\n",
    "    updated_strings_df['1'] = 'none'\n",
    "    updated_strings_df['2'] = 'none'\n",
    "    updated_strings_df['3'] = 'none'\n",
    "    \n",
    "    # optionally use keyword extract\n",
    "    # updated_strings_df['Original_Value']= 'extract'\n",
    "    \n",
    "        # Replace NaN values in 'Original_Value' with 1\n",
    "    updated_strings_df['Original_Value'].fillna(1, inplace=True)\n",
    "    \n",
    "    return updated_strings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51877a-2850-48a7-ba97-b554f8a79afb",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb5f084-dae4-4c9b-a0ef-dae15beab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "file_path = '/home/paulc600/scratch/prms_final/par_files/12_myparam.param' # ensure its the updated par file\n",
    "output_file_path = './data/paired_myparam.param'\n",
    "ostin_filepath= './written_ostIn.txt'\n",
    "target_ID= 24\n",
    "upstream_ID_1= 24 # to remove hrus upstream of a previous gauge\n",
    "upstream_ID_2= 429 # to remove hrus upstream of a previous gauge\n",
    "target_strings = ['adjmix_rain', 'carea_max', 'cecn_coef','emis_noppt','fastcoef_lin','freeh2o_cap',\n",
    "                  'gwflow_coef','jh_coef','mann_n','potet_sublim','rad_trncf','radmax','rain_cbh_adj',\n",
    "                  'slowcoef_sq','smidx_coef', 'snarea_thresh','snowinfil_max','snow_cbh_adj',\n",
    "                  'soil2gw_max','soil_moist_max','soil_rechr_max_frac','ssr2gw_exp','ssr2gw_rate',\n",
    "                  'tmax_allrain_offset','tmax_allsnow','tmax_cbh_adj','tmin_cbh_adj']\n",
    "min_coef_variation = 0\n",
    "max_coef_variation = 20\n",
    "updated_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f0e455-9ae8-4c9c-8ea7-e69d34226e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter ranges\n",
    "low_bound= [0,-9999, 0, 0.757,-9999, 0.01,-9999,0, 0.001, 0.06,-9999, 0, 0, 0, 0.0001,-9999, \n",
    "            0, 0, -9999,-9999,-9999,0 , 0.01,0,0, 0, 0]\n",
    "upper_bound= [2, -9999, 5.5, 1,-9999, 0.1,-9999,2, 0.15, 0.4,-9999, 2, 2, 1, 0.8,-9999, \n",
    "            20, 2, -9999, -9999,-9999,3,0.8, 2,2, 2, 2]\n",
    "used= [1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, \n",
    "                   0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0] # subtracting tmax_allsnow and cecn because of tiny coef of var\n",
    "data = {\n",
    "    'low_bound': low_bound,\n",
    "    'upper_bound': upper_bound\n",
    "}\n",
    "# Create a DataFrame with target_strings as the index\n",
    "ostin_table = pd.DataFrame(data, index=target_strings)\n",
    "use_table= pd.DataFrame(used, index=target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "467aee43-fc69-4301-9e0d-c0072189c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = read_and_extract_multiple(file_path, target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c470ac0-1909-4c62-bbd5-7249f5378f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = calculate_metrics(values, target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc777535-f56f-4a14-bd0d-d8c40ac230f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_variables_coef_variation = filter_variables_coef_variation(result_df, min_coef_variation, max_coef_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c89386-5cb9-451f-815a-1c03e4122e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variables based on coef of variation:\n",
      "                     Count       Mean   Std Dev    Max-Min  Coef of Var\n",
      "adjmix_rain          10908   1.016365  0.204827   0.800000     0.201529\n",
      "carea_max              908   0.462356  0.252819   1.886716     0.546806\n",
      "cecn_coef            10908   4.999627  0.443873   2.000000     0.088781\n",
      "emis_noppt             908   0.832001  0.077010   0.242883     0.092560\n",
      "fastcoef_lin           908   0.191484  0.133472   0.514845     0.697040\n",
      "freeh2o_cap            908   0.052786  0.027615   0.090000     0.523143\n",
      "gwflow_coef            908   0.020989  0.018417   0.092292     0.877490\n",
      "jh_coef              10908   0.006254  0.002329   0.014163     0.372375\n",
      "mann_n                 448   0.052369  0.040137   0.148844     0.766438\n",
      "potet_sublim           908   0.469392  0.075677   0.537019     0.161223\n",
      "rad_trncf              908   0.667957  0.273844   1.000000     0.409972\n",
      "radmax               10908   0.759153  0.141742   0.492738     0.186711\n",
      "rain_cbh_adj         10908   0.732960  0.347189   2.500000     0.473680\n",
      "slowcoef_sq            908   0.355811  0.292839   1.000000     0.823017\n",
      "smidx_coef             908   0.019470  0.092222   0.797997     4.736545\n",
      "snarea_thresh          908  14.617023  8.123705  41.031968     0.555770\n",
      "snowinfil_max          908   5.680001  5.338813  19.981527     0.939932\n",
      "snow_cbh_adj         10908   1.400610  0.515686   2.500000     0.368187\n",
      "soil2gw_max            908   0.227774  0.159264   0.583265     0.699220\n",
      "soil_moist_max         908   3.654633  2.850470   9.997445     0.779961\n",
      "soil_rechr_max_frac    908   0.692621  0.300670   0.978949     0.434105\n",
      "ssr2gw_exp             908   1.382649  0.757127   2.994677     0.547592\n",
      "ssr2gw_rate            908   0.359489  0.228522   0.788930     0.635685\n",
      "tmax_allrain_offset  10908   7.114998  2.874634  10.000000     0.404025\n",
      "tmax_allsnow         10908  33.753586  3.450426  13.476460     0.102224\n",
      "tmax_cbh_adj         10908  -1.610690  1.540646   6.000000     0.956513\n",
      "tmin_cbh_adj         10908  -0.888870  1.806825   6.000000     2.032721\n"
     ]
    }
   ],
   "source": [
    "# Display the selected variables\n",
    "print(\"Selected variables based on coef of variation:\")\n",
    "print(filter_variables_coef_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6092cca5-acdc-47b8-888e-861f2db42e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_variables_coef_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "898b9fd8-4636-4623-9598-8308a45e962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_by_use= filter_variables_by_use(result_df, use_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69116ced-e813-4b27-ac4c-6fe649efe7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_variables_by_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6284d91c-144e-4440-add3-2934b4a00795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variables based on use in StreamFlow Cal:\n",
      "                     Count       Mean   Std Dev    Max-Min  Coef of Var\n",
      "adjmix_rain          10908   1.016365  0.204827   0.800000     0.201529\n",
      "cecn_coef            10908   4.999627  0.443873   2.000000     0.088781\n",
      "emis_noppt             908   0.832001  0.077010   0.242883     0.092560\n",
      "freeh2o_cap            908   0.052786  0.027615   0.090000     0.523143\n",
      "gwflow_coef            908   0.020989  0.018417   0.092292     0.877490\n",
      "potet_sublim           908   0.469392  0.075677   0.537019     0.161223\n",
      "smidx_coef             908   0.019470  0.092222   0.797997     4.736545\n",
      "soil2gw_max            908   0.227774  0.159264   0.583265     0.699220\n",
      "soil_moist_max         908   3.654633  2.850470   9.997445     0.779961\n",
      "soil_rechr_max_frac    908   0.692621  0.300670   0.978949     0.434105\n",
      "tmax_allrain_offset  10908   7.114998  2.874634  10.000000     0.404025\n",
      "tmax_allsnow         10908  33.753586  3.450426  13.476460     0.102224\n"
     ]
    }
   ],
   "source": [
    "# Display the selected variables\n",
    "print(\"Selected variables based on use in StreamFlow Cal:\")\n",
    "print(selected_variables_by_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27a00b-dbe6-4863-9721-2f0716be13f1",
   "metadata": {},
   "source": [
    "Paired Parameter File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31de3d4a-cf42-40f6-84c8-e2912816ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Par File into a numpy array to convert to Paired    \n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "par_paired = np.array([line.strip() for line in lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0952a-f50c-4e31-9691-f699f3b9386e",
   "metadata": {},
   "source": [
    "Define river segment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d45c46a4-34c9-4fa4-aa0a-422f5fc80528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find upstream nseg index values which can be used to write nseg parameters\n",
    "nseg= find_nseg(file_path, target_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a782b9f-5568-4f48-8025-a7b0b6c4df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5945c6-971d-4a0e-8c6a-31ee8b7f1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find previous or upstream gauge\n",
    "# previous_nseg1= find_nseg(file_path, upstream_ID_1)\n",
    "# previous_nseg2= find_nseg(file_path, upstream_ID_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6ff57de-0d6e-4da2-a502-27fe80f29195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous gauge\n",
    "# nseg= remove_duplicates(nseg, previous_nseg1)\n",
    "# nseg= remove_duplicates(nseg, previous_nseg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b94b4603-8089-48c9-be28-e681eb7c9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine gauge nseg lists\n",
    "#nseg= combine_nseg(nseg,nseg)\n",
    "# nseg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65517527-a27c-4f17-8755-276b15c2c922",
   "metadata": {},
   "source": [
    "Find nhru and write paired parameter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88959f2b-512d-4de4-b71d-e173fbe8f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhru= find_nhru(file_path, nseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c073f614-dd95-4733-8411-9fc0a28b2ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nhru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55bae9c2-e1e3-4862-86ae-aebd3b258f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_paired= write_nseg(par_paired, filter_variables_coef_variation, nseg, updated_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fdb244a-a18f-4374-a6c6-c970591c80ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "par_paired= write_nhru(par_paired, filter_variables_coef_variation, nhru, updated_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d1afc25-f111-4496-8fea-74f39509c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# par_paired= write_nmonths(par_paired, filter_variables_coef_variation, nhru, updated_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f01e4efc-7f8a-409f-9a03-e5bec9af00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly, monthly_paired= write_alt_input(par_paired, filter_variables_coef_variation, nhru, updated_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0c707d2-7fe1-4d0c-8913-90f6b587a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrames to text files\n",
    "monthly.to_csv('./data/monthly.txt', sep=' ', index=True)\n",
    "monthly_paired.to_csv('./data/paired_monthly.txt', sep=' ', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34aad34f-2775-4807-9b7d-c79f04c0f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace inflow to segment with observed flow\n",
    "# par_paired=obsin_segment(par_paired, 21, 0)\n",
    "# par_paired=obsin_segment(par_paired, 353, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89859a08-9c45-40f2-a4cf-8aa785c03731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parpaired to the specified file path\n",
    "np.savetxt(output_file_path, par_paired, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25dde548-d03e-49b2-b218-49cd28e35aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e75ae-f860-47d9-853e-e55f43f0d9ed",
   "metadata": {},
   "source": [
    "Write ostin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fcb7f36-5c32-4c91-855c-fb572fd9d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ostin= write_ostin_table(ostin_table, updated_strings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5795060-25c7-4a66-bc15-20f85628e242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Value</th>\n",
       "      <th>Low_Bound</th>\n",
       "      <th>Upper_Bound</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Updated_String</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>___mann_n28_A</th>\n",
       "      <td>0.037624</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___mann_n29_A</th>\n",
       "      <td>0.140344</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___mann_n25_A</th>\n",
       "      <td>0.083048</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___mann_n24_A</th>\n",
       "      <td>0.139748</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___carea_max5_A</th>\n",
       "      <td>1.699380</td>\n",
       "      <td>1.359504</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___adjust_tmin_cbh_adj_4_A</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___adjust_tmin_cbh_adj_9_A</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___adjust_tmin_cbh_adj_20_A</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___adjust_tmin_cbh_adj_7_A</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___adjust_tmin_cbh_adj_15_A</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original_Value  Low_Bound  Upper_Bound     1  \\\n",
       "Updated_String                                                              \n",
       "___mann_n28_A                      0.037624   0.001000         0.15  none   \n",
       "___mann_n29_A                      0.140344   0.001000         0.15  none   \n",
       "___mann_n25_A                      0.083048   0.001000         0.15  none   \n",
       "___mann_n24_A                      0.139748   0.001000         0.15  none   \n",
       "___carea_max5_A                    1.699380   1.359504         2.00  none   \n",
       "...                                     ...        ...          ...   ...   \n",
       "___adjust_tmin_cbh_adj_4_A         1.000000   0.000000         2.00  none   \n",
       "___adjust_tmin_cbh_adj_9_A         1.000000   0.000000         2.00  none   \n",
       "___adjust_tmin_cbh_adj_20_A        1.000000   0.000000         2.00  none   \n",
       "___adjust_tmin_cbh_adj_7_A         1.000000   0.000000         2.00  none   \n",
       "___adjust_tmin_cbh_adj_15_A        1.000000   0.000000         2.00  none   \n",
       "\n",
       "                                2     3  \n",
       "Updated_String                           \n",
       "___mann_n28_A                none  none  \n",
       "___mann_n29_A                none  none  \n",
       "___mann_n25_A                none  none  \n",
       "___mann_n24_A                none  none  \n",
       "___carea_max5_A              none  none  \n",
       "...                           ...   ...  \n",
       "___adjust_tmin_cbh_adj_4_A   none  none  \n",
       "___adjust_tmin_cbh_adj_9_A   none  none  \n",
       "___adjust_tmin_cbh_adj_20_A  none  none  \n",
       "___adjust_tmin_cbh_adj_7_A   none  none  \n",
       "___adjust_tmin_cbh_adj_15_A  none  none  \n",
       "\n",
       "[212 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ostin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "704dfb3d-8d89-420f-9522-2a5421853728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom header and footer strings\n",
    "top_half = \"\"\"#Basic Configuration for Ostrich Program\n",
    "\n",
    "#Essential variables\n",
    "ProgramType ParallelDDS\n",
    "ModelExecutable ./adj_obsin_calibrate_prms.py\n",
    "ModelSubdir mod_\n",
    "ObjectiveFunction GCOP\n",
    "OstrichWarmStart no\n",
    "\n",
    "#Template File Configuration\n",
    "BeginFilePairs\n",
    "data/paired_myparam.param ; data/myparam.param\n",
    "data/paired_monthly.txt ; data/monthly.txt\n",
    "EndFilePairs\n",
    "\n",
    "BeginExtraFiles\n",
    "EndExtraFiles\n",
    "\n",
    "BeginExtraDirs\n",
    "data\n",
    "EndExtraDirs\n",
    "\n",
    "\n",
    "BeginParams\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e20db137-2ab2-48a1-ae30-b97d48f23c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_half= \"\"\"EndParams\n",
    "\n",
    "#Output variables to analyze\n",
    "BeginResponseVars\n",
    "#name   filename            keyword         line    col     token\n",
    "KGE ./data/average_kge.txt ; OST_NULL 0 1 ' '\n",
    "EndResponseVars\n",
    "\n",
    "BeginGCOP\n",
    "CostFunction KGE\n",
    "PenaltyFunction APM\n",
    "EndGCOP\n",
    "\n",
    "#DDS algorithm setup\n",
    "BeginParallelDDSAlg\n",
    "PerturbationValue 0.2\n",
    "MaxIterations 50000\n",
    "UseInitialParamValues\n",
    "EndParallelDDSAlg\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "043c57ef-f651-4a89-8f9c-22ee84908513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a text file with header and footer strings\n",
    "with open(ostin_filepath, 'w') as file:\n",
    "    file.write(top_half + '\\n')\n",
    "    ostin.to_csv(file, header=False, sep=' ')  # You can change the separator if needed\n",
    "    file.write(bottom_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea6f66-3092-4f83-82e1-4f1fe0e8aca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easymore-env",
   "language": "python",
   "name": "easymore-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
