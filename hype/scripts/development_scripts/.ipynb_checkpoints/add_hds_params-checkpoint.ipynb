{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4af896-1a4d-4b84-b82b-38b728b21acf",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "This script uses the output from Mohamed's R script containing depressionDepth_m, depressionAreaFrac and deprCatchAreaFrac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73d7958-8ef9-4412-9538-4d72308d53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f4691-ba82-4cfe-bed0-25c2bd6066d4",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ca66b5-341e-485c-9784-5b0e3e8a42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = pd.read_csv('../../model/add_HDS_GeoData.txt', header=0, index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc83ef2-e219-460e-b629-21e64de37989",
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_params= pd.read_csv('../../geospacial/depressions/Milk_HDS_parameters.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0901ab44-926b-48b7-82e1-d1571d94aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile= gpd.read_file('../../geospacial/shapefiles/modified_shapefiles/Modified_SMMcat.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6cfc6b-fc93-4634-b952-3c209da7de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ilake slcs for the Milk\n",
    "milk_ilake= 'SLC_62'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aaa57d-6a55-4129-b8ac-72e8b360de83",
   "metadata": {},
   "source": [
    "### Remove HDS from St. Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5594586c-932b-4f93-bb61-07ea54927d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata.index= geodata.index.astype(int)\n",
    "hds_params.index= hds_params.index.astype(int)\n",
    "shapefile['hru_nhm']= shapefile['hru_nhm'].astype(int)\n",
    "shapefile['seg_nhm']= shapefile['seg_nhm'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d669b865-ba9c-4a49-954a-9db9ec876f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary from the 'hru_nhm' and 'seg_nhm' columns\n",
    "hru_seg_dict = dict(zip(shapefile['hru_nhm'], shapefile['seg_nhm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e2a283-493c-4e93-b3d4-12be5e61f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace new params index with river seg\n",
    "hds_params.index = hds_params.index.map(hru_seg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d534551c-701b-4b21-ab89-f814c871dd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depressionDepth_m</th>\n",
       "      <th>depressionAreaFrac</th>\n",
       "      <th>deprCatchAreaFrac</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58183</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58184</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58185</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58186</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58188</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58671</th>\n",
       "      <td>0.774399</td>\n",
       "      <td>0.264501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58672</th>\n",
       "      <td>1.559696</td>\n",
       "      <td>0.236104</td>\n",
       "      <td>0.980646</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58673</th>\n",
       "      <td>2.651541</td>\n",
       "      <td>0.287069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58674</th>\n",
       "      <td>0.527663</td>\n",
       "      <td>0.112770</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58675</th>\n",
       "      <td>1.179411</td>\n",
       "      <td>0.140129</td>\n",
       "      <td>0.813159</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       depressionDepth_m  depressionAreaFrac  deprCatchAreaFrac  Unnamed: 4\n",
       "subid                                                                      \n",
       "58183           0.000000            0.000000           0.000000         NaN\n",
       "58184           0.000000            0.000000           0.000000         NaN\n",
       "58185           0.000000            0.000000           0.000000         NaN\n",
       "58186           0.000000            0.000000           0.000000         NaN\n",
       "58188           0.000000            0.000000           0.000000         NaN\n",
       "...                  ...                 ...                ...         ...\n",
       "58671           0.774399            0.264501           1.000000         NaN\n",
       "58672           1.559696            0.236104           0.980646         NaN\n",
       "58673           2.651541            0.287069           1.000000         NaN\n",
       "58674           0.527663            0.112770           1.000000         NaN\n",
       "58675           1.179411            0.140129           0.813159         NaN\n",
       "\n",
       "[473 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hds_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb324a-e984-4865-b941-0260946fe6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the last column using its name\n",
    "hds_params = new_params.drop(columns=['Unnamed: 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d04fec-2eb4-48c3-8b0b-15f0673acacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Creating a DiGraph out of `df` object\n",
    "riv_graph = nx.from_pandas_edgelist(geodata.reset_index(), source='subid', target='maindown', create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823cd1f0-f2ce-4845-82cc-237067ac14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find St. Mary Segments\n",
    "stmary = [58183]\n",
    "stmary.extend(nx.ancestors(riv_graph, 58183))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78afb4e4-6ac4-4d3b-bc21-1309424826f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stmary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f4762b-d673-4402-9869-ef1490961229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows containing info on St Mary so only the Milk is \n",
    "hdsdepths_filtered = hds_depths[~hds_depths['seg_nhm'].isin(stmary)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351adec3-7739-421c-9171-1cbf4e0e9725",
   "metadata": {},
   "source": [
    "### Format GeoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be677de0-ff90-4af0-99b3-906183402762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'seg_nhm' as the index in hdsdepths_filtered\n",
    "hdsdepths_filtered = hdsdepths_filtered.set_index('seg_nhm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf64cbb-0936-46ad-bd2f-258adeec133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge based on the index\n",
    "merged_geodata = geodata.merge(hdsdepths_filtered[['_count', '_mean']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025b89e0-5535-4142-ae06-5f4ab3ff947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0 in the new columns\n",
    "merged_geodata['_count'].fillna(0, inplace=True)\n",
    "merged_geodata['_mean'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9e6039-fd31-433a-9f37-1c92b3bcd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename '_mean' to 'hds_depth'\n",
    "merged_geodata.rename(columns={'_mean': 'hds_depth'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eaf025e-47c6-4c49-90ae-b99c36f59cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First column index with 'SLC': 7\n",
      "Last column index with 'SLC': 123\n"
     ]
    }
   ],
   "source": [
    "# find index of first and last SLC\n",
    "filtered_columns = merged_geodata.filter(like='SLC').columns\n",
    "\n",
    "if len(filtered_columns) > 0:\n",
    "    first_slc_index = merged_geodata.columns.get_loc(filtered_columns[0])\n",
    "    last_slc_index = merged_geodata.columns.get_loc(filtered_columns[-1])\n",
    "    print(\"First column index with 'SLC':\", first_slc_index)\n",
    "    print(\"Last column index with 'SLC':\", last_slc_index)\n",
    "else:\n",
    "    print(\"No columns with 'SLC' in the name found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be7ccc4-2c48-4a4c-9832-f22349ed05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count to area\n",
    "merged_geodata['_count'] = merged_geodata['_count'] * (19.88 * 31.21) # multiply by DEM resolution from properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf61cab8-24fd-4613-82c4-2ad80ecbf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'hds_frac' which is equal to count/area\n",
    "merged_geodata['hds_frac'] = merged_geodata['_count'] / merged_geodata['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249fed62-cf3c-4216-8b8d-2d9015b9dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_geodata[milk_ilake] = merged_geodata['hds_frac']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47fa10-c086-440d-96cb-4fe649e7448d",
   "metadata": {},
   "source": [
    "### Adjust Milk SLC fractions based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e790211-347b-4b7a-b388-4db419a6f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row\n",
    "for index, row in merged_geodata.iterrows():\n",
    "    \n",
    "        # Calculate the sum of slc columns for the current row\n",
    "        sum_of_columns_row = row.iloc[first_slc_index:last_slc_index+1].sum()\n",
    "\n",
    "        # Subtract the value in the milk_ilake column for the current row\n",
    "        result_row = sum_of_columns_row - row[milk_ilake]\n",
    "\n",
    "        # Calculate the scaling factor based on the formula\n",
    "        scaling_factor = 1 - row[milk_ilake]\n",
    "\n",
    "        # Iterate through SLC_1 to SLC_n columns and update values\n",
    "        for col in merged_geodata.columns:\n",
    "            if col.startswith('SLC_') and col != milk_ilake:\n",
    "                merged_geodata.loc[index, col] = (row[col] / result_row) * scaling_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b96b55-3452-4fdd-aa9b-81eb339efca5",
   "metadata": {},
   "source": [
    "### Check that SLCs still sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5b59185-1e39-4df6-adc4-fb63f0ae529a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check sums of rows\n",
    "for index, row in merged_geodata.iterrows():\n",
    "    # Calculate the sum of values in the specified columns\n",
    "    row_sum = row.iloc[first_slc_index:(last_slc_index + 1)].sum()\n",
    "\n",
    "    # Check if the sum is approximately equal to 1\n",
    "    if not np.isclose(row_sum, 1, rtol=1e-6):\n",
    "        print(f\"Warning: Row {index} does not sum to 1 (Sum: {row_sum})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa82af51-76dc-4e60-9b10-8869ce78dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Insert 'hds_depth' column just before the first 'SLC' column\n",
    "    columns = list(merged_geodata.columns)\n",
    "    columns.insert(first_slc_index, columns.pop(columns.index('hds_depth')))\n",
    "    merged_geodata = merged_geodata[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f8b95d-eb40-411a-b4e7-23fa99251bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the 'hds_frac' column\n",
    "merged_geodata.drop(columns=['hds_frac'], inplace=True)\n",
    "# Delete the '_count' column\n",
    "merged_geodata.drop(columns=['_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b2b231a-81b7-4e04-ba59-b9e6afd19078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GeoDataFrame as a tab-separated text file\n",
    "merged_geodata.to_csv('../../model/depression_analysis/dimension_GeoData.txt', sep='\\t', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
