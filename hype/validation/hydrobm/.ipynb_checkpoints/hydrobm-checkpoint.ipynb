{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8857fb03-91fb-410f-b160-8a7c2b5fc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import hydrobm\n",
    "import matplotlib.pyplot as plt\n",
    "from hydrobm.calculate import calc_bm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d5df6-f163-4a45-923b-8341507fbea4",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c711c4bb-f6ee-4597-b6f4-d24152fac5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file paths\n",
    "performance_metrics= '../../model/model_versions/v_7/v7_1/performance_metrics.csv'\n",
    "\n",
    "pobs= '../../model/model_versions/v_7/v7_1/Pobs.txt'\n",
    "\n",
    "qobs= '../../model/model_versions/v_7/v7_1/Qobs.txt'\n",
    "\n",
    "geodata= '../../model/model_versions/v_7/v7_1/GeoData.txt'\n",
    "\n",
    "output_dir= './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c1c823-dd1d-45ae-92bd-22cbca6370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calibration and validation periods\n",
    "calibration_ranges = [('1980-10-01', '1984-09-30'),\n",
    "               ('1989-10-01', '1998-09-30'),\n",
    "               ('2003-10-01', '2007-09-30'),\n",
    "               ('2012-10-01', '2015-09-30')]\n",
    "\n",
    "validation_ranges = [('1984-10-01', '1989-09-30'),\n",
    "               ('1998-10-01', '2003-09-30'),\n",
    "               ('2007-10-01', '2012-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df74286-33b3-4a6a-ae6a-88f2fe89c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the benchmarks and metrics to calculate\n",
    "benchmarks = [\n",
    "        # Streamflow benchmarks\n",
    "        \"mean_flow\",\n",
    "        \"median_flow\",\n",
    "        \"annual_mean_flow\",\n",
    "        \"annual_median_flow\",\n",
    "        \"monthly_mean_flow\",\n",
    "        \"monthly_median_flow\",\n",
    "        \"daily_mean_flow\",\n",
    "        \"daily_median_flow\",\n",
    "\n",
    "#         # Long-term rainfall-runoff ratio benchmarks\n",
    "#         \"rainfall_runoff_ratio_to_all\",\n",
    "#         \"rainfall_runoff_ratio_to_annual\",\n",
    "#         \"rainfall_runoff_ratio_to_monthly\",\n",
    "#         \"rainfall_runoff_ratio_to_daily\",\n",
    "#         \"rainfall_runoff_ratio_to_timestep\",\n",
    "\n",
    "# #         # Short-term rainfall-runoff ratio benchmarks\n",
    "#         \"monthly_rainfall_runoff_ratio_to_monthly\",\n",
    "#         \"monthly_rainfall_runoff_ratio_to_daily\",\n",
    "#         \"monthly_rainfall_runoff_ratio_to_timestep\",\n",
    "\n",
    "#         # Schaefli & Gupta (2007) benchmarks\n",
    "#         \"scaled_precipitation_benchmark\",  # equivalent to \"rainfall_runoff_ratio_to_daily\"\n",
    "#         \"adjusted_precipitation_benchmark\",\n",
    "#         \"adjusted_smoothed_precipitation_benchmark\",\n",
    "    ]\n",
    "\n",
    "metrics = ['nse', 'kge'] # could also use 'mse', 'rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf232f-3511-416a-bcb4-b4aa6b1db4fd",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551ff40-9cae-4a5d-a324-afde8627035d",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ca5205-4502-494a-a442-3625fcdd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "performance_metrics= pd.read_csv(performance_metrics, index_col=0)\n",
    "pobs= pd.read_csv(pobs, index_col=0, sep='\\t') \n",
    "qobs= pd.read_csv(qobs, index_col=0, sep='\\t') \n",
    "geodata= pd.read_csv(geodata, index_col=0, sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08cde0f-f61a-4b79-9bab-d566680891cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert index to datetime\n",
    "pobs.index = pd.to_datetime(pobs.index)\n",
    "qobs.index = pd.to_datetime(qobs.index)\n",
    "\n",
    "# Set index to int\n",
    "geodata.index = geodata.index.astype(int)\n",
    "\n",
    "# Convert column headers to integers\n",
    "pobs.columns = pobs.columns.astype(int)\n",
    "qobs.columns = qobs.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5b6032-9d67-4661-b58c-031d769ce284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics index\n",
    "subbasin_ids= performance_metrics.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e826fdce-5477-4cb3-aca6-7863d4fb560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ranges to Pandas Timestamps\n",
    "calibration_ranges = [(pd.Timestamp(start), pd.Timestamp(end)) for start, end in calibration_ranges]\n",
    "validation_ranges = [(pd.Timestamp(start), pd.Timestamp(end)) for start, end in validation_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96818d90-68df-49d6-9be2-76b99034a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "riv_graph = nx.DiGraph()\n",
    "\n",
    "# Add edges from DataFrame\n",
    "for idx, row in geodata.iterrows():\n",
    "    if row['maindown'] != '0':  # Skip if maindown is '0'\n",
    "        riv_graph.add_edge(idx, row['maindown'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bea758c-f9eb-4fb9-80d7-5f77354ffb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with nan in streamflow\n",
    "qobs.replace(-9999, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158dbdcd-24b1-45ff-b081-04b338b304ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert streamflow from m3/s to m3\n",
    "qobs= qobs * 86400 # s / day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9093576b-90c3-4159-ba6c-b3ca4730d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set area column to numeric\n",
    "geodata['area'] = pd.to_numeric(geodata['area'])\n",
    "\n",
    "# Create dictionary with subbasin ID and area\n",
    "area_dict = geodata['area'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88ba6f7-5938-4418-8b45-644bcdda3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pobs from mm to m3\n",
    "pobs= pobs / 1000 # mm to m\n",
    "\n",
    "# Multiply each column in pobs by the corresponding area value in area_dict\n",
    "for col in pobs.columns:\n",
    "    if col in area_dict:\n",
    "        pobs[col] *= area_dict[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27bd18-bc50-471e-bdde-ab9ba34939fa",
   "metadata": {},
   "source": [
    "### Calculate Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0343a722-672c-4071-99fa-7d154c44e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the four best benchmarks for plotting\n",
    "def top_n_indices_and_values(values_list, n=4):\n",
    "    arr = np.array(values_list) # numpy array\n",
    "    nan_idx = np.where(np.isnan(arr)) # find nan values\n",
    "    arr_sort = arr.argsort() # sort the full array, nans go at the end\n",
    "    arr_sort = arr_sort[~np.isin(arr_sort, nan_idx)] # remove nans\n",
    "    indices = arr_sort[-n:] # get the top n indices\n",
    "    values = arr[indices] # get the values\n",
    "    return indices.tolist(), values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6368ca-4d47-43b3-a641-084f7a94fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list to store the messages\n",
    "worse_than_benchmark_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8878cc6-caf2-43d3-a3bb-5eba34b81d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "58643_nse_val is worse than benchmark in mean_flow\n",
      "58643_nse_val is worse than benchmark in median_flow\n",
      "58643_nse_val is worse than benchmark in annual_mean_flow\n",
      "58643_nse_val is worse than benchmark in annual_median_flow\n",
      "58643_nse_val is worse than benchmark in monthly_mean_flow\n",
      "58643_nse_val is worse than benchmark in monthly_median_flow\n",
      "58643_nse_val is worse than benchmark in daily_mean_flow\n",
      "58643_nse_val is worse than benchmark in daily_median_flow\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "58363_nse_val is worse than benchmark in mean_flow\n",
      "58363_nse_val is worse than benchmark in median_flow\n",
      "58363_nse_val is worse than benchmark in annual_mean_flow\n",
      "58363_nse_val is worse than benchmark in annual_median_flow\n",
      "58363_nse_val is worse than benchmark in monthly_mean_flow\n",
      "58363_nse_val is worse than benchmark in monthly_median_flow\n",
      "58363_nse_val is worse than benchmark in daily_mean_flow\n",
      "58363_nse_val is worse than benchmark in daily_median_flow\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "58308_nse_val is worse than benchmark in mean_flow\n",
      "58308_nse_val is worse than benchmark in monthly_mean_flow\n",
      "58308_nse_val is worse than benchmark in monthly_median_flow\n",
      "58308_nse_val is worse than benchmark in daily_mean_flow\n",
      "58308_nse_val is worse than benchmark in daily_median_flow\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "58290_nse_val is worse than benchmark in monthly_mean_flow\n",
      "58290_nse_val is worse than benchmark in monthly_median_flow\n",
      "58290_nse_val is worse than benchmark in daily_median_flow\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "58213_kge_cal is worse than benchmark in daily_mean_flow\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each subbasin in performance_metrics\n",
    "for subbasin in performance_metrics.index:\n",
    "\n",
    "    # Find upstream segments for the given subbasin\n",
    "    upstream_segments = list(nx.ancestors(riv_graph, subbasin))\n",
    "\n",
    "    # Sum upstream precipitation\n",
    "    precipitation_sum = pd.DataFrame(pobs[upstream_segments].sum(axis=1), columns=['precipitation'])\n",
    "\n",
    "    # Create hydrobm input dataframe\n",
    "    bm_input = pd.DataFrame({\n",
    "        'streamflow': qobs[subbasin],  # Streamflow for the given subbasin\n",
    "        'precipitation': precipitation_sum['precipitation'],  # Sum of upstream precipitation\n",
    "    })\n",
    "\n",
    "    # Create the cal_mask column\n",
    "    bm_input['cal_mask'] = bm_input.index.to_series().apply(\n",
    "        lambda x: any(pd.to_datetime(start) <= x <= pd.to_datetime(end) for start, end in calibration_ranges)\n",
    "    )\n",
    "\n",
    "    # Create the val_mask column\n",
    "    bm_input['val_mask'] = bm_input.index.to_series().apply(\n",
    "        lambda x: any(pd.to_datetime(start) <= x <= pd.to_datetime(end) for start, end in validation_ranges)\n",
    "    )\n",
    "\n",
    "    # Calculate the benchmarks and scores\n",
    "    benchmark_flows, scores = calc_bm(\n",
    "        bm_input,\n",
    "\n",
    "        # Time period selection\n",
    "        bm_input['cal_mask'],\n",
    "        val_mask=bm_input['val_mask'],\n",
    "\n",
    "        # Variable names in 'data'\n",
    "        precipitation=\"precipitation\",\n",
    "        streamflow=\"streamflow\",\n",
    "\n",
    "        # Benchmark choices\n",
    "        benchmarks=benchmarks,\n",
    "        metrics=metrics,\n",
    "        optimization_method=\"brute_force\",\n",
    "\n",
    "        # Snow model inputs\n",
    "        calc_snowmelt=False,\n",
    "        temperature=\"temperature_2m_mean\",\n",
    "        snowmelt_threshold=0.0,\n",
    "        snowmelt_rate=3.0,\n",
    "    )\n",
    "\n",
    "    # Get top 4 KGE scores and corresponding benchmarks\n",
    "    idx, vals = top_n_indices_and_values(scores['kge_val'], 4)\n",
    "    top_benchmarks = [scores['benchmarks'][id] for id in idx]\n",
    "    top_kge_vals = [scores['kge_val'][id] for id in idx]\n",
    "\n",
    "    # Reformat the scores for cleaner saving \n",
    "    col_names = scores.pop(\"benchmarks\", None)\n",
    "    df = pd.DataFrame(scores, index=col_names)\n",
    "    df = df.T\n",
    "\n",
    "        # Add the model_results column with performance metrics for the subbasin\n",
    "    df['model_results'] = None  # Initialize the column\n",
    "    df.loc['kge_cal', 'model_results'] = performance_metrics.loc[subbasin, 'Cal KGE']\n",
    "    df.loc['kge_val', 'model_results'] = performance_metrics.loc[subbasin, 'Val KGE']\n",
    "    df.loc['nse_cal', 'model_results'] = performance_metrics.loc[subbasin, 'Cal NSE']\n",
    "    df.loc['nse_val', 'model_results'] = performance_metrics.loc[subbasin, 'Val NSE']\n",
    "\n",
    "    # Save the DataFrame to CSV in the output directory\n",
    "    df_filepath = os.path.join(output_dir, f\"subbasin_{subbasin}_scores.csv\")\n",
    "    df.to_csv(df_filepath)\n",
    "    \n",
    "        # Plot streamflow along with the four best benchmarks\n",
    "    fig, ax = plt.subplots(4, 1, figsize=(14, 14))\n",
    "    for i, (bm, kge) in enumerate(zip(top_benchmarks, top_kge_vals)):\n",
    "        bm_input['streamflow'].plot(ax=ax[i], linewidth=2, label='streamflow')\n",
    "        benchmark_flows[f'bm_{bm}'].plot(ax=ax[i], label=bm)\n",
    "        ax[i].legend(loc='upper left')\n",
    "        ax[i].set_title(f\"{subbasin}_{bm}_(BM Val KGE: {kge:.2f}, Model Val KGE: {df.loc['kge_val', 'model_results']:.2f})\")\n",
    "        ax[i].set_xlabel('')  # Drops 'Date'\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure to the output directory\n",
    "    fig_filepath = os.path.join(output_dir, f\"subbasin_{subbasin}_benchmark_plot.png\")\n",
    "    fig.savefig(fig_filepath)\n",
    "    plt.close(fig)  # Close the figure after saving to free memory\n",
    "    \n",
    "    # Iterate through each row in df\n",
    "    for idx, row in df.iterrows():\n",
    "        # Loop through all columns except 'model_results'\n",
    "        for col in df.columns:\n",
    "            if col != 'model_results' and row['model_results'] < row[col]:\n",
    "                message = f\"{subbasin}_{idx} is worse than benchmark in {col}\"\n",
    "                print(message)  # Print the message\n",
    "                worse_than_benchmark_list.append(message)  # Append the message to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f2b067-d811-4bdf-8936-52014cbeaec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the list to ./worse_than_benchmark_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the file in the output_dir\n",
    "output_file_path = os.path.join(output_dir, 'worse_than_benchmark_list.txt')\n",
    "\n",
    "# Save the list to a text file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for line in worse_than_benchmark_list:\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "print(f\"Saved the list to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73a3c4-e044-4ee1-9ce0-b979556338a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
